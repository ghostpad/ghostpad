name: koboldai
channels:
  - pytorch
  - nvidia/label/cuda-11.8.0
  - conda-forge
  - defaults
dependencies:
  - colorama
  - flask=2.3.3
  - flask-socketio=5.3.2
  - flask-session=0.5.0
  - python-socketio=5.7.2
  - pytorch=2.0.*
  - python=3.10.*
  - pytorch-cuda=11.8
  - cuda-nvcc=11.8
  - cuda-libraries-dev=11.8
  - eventlet=0.33.3
  - dnspython=2.2.1
  - markdown
  - bleach=4.1.0
  - pip
  - git=2.35.1
  - protobuf
  - marshmallow>=3.13
  - apispec-webframeworks
  - loguru
  - termcolor
  - Pillow
  - psutil
  - ffmpeg
  - pip:
    - flask-cloudflared==0.0.10
    - flask-ngrok
    - flask-cors
    - lupa==2.0
    - transformers[sentencepiece]==4.34.0
    - Werkzeug==2.3.7
    - huggingface_hub==0.16.4
    - optimum[onnxruntime]==1.13.2
    - safetensors==0.3.3
    - accelerate==0.21.0
    - git+https://github.com/VE-FORBRYDERNE/mkultra
    - flask-session
    - ansi2html
    - flask_compress
    - ijson
    - bitsandbytes==0.40.0.post4; sys_platform == 'linux'
    - https://github.com/jllllll/bitsandbytes-windows-webui/releases/download/wheels/bitsandbytes-0.40.0.post4-py3-none-win_amd64.whl; sys_platform == 'win32'
    - ftfy
    - pydub
    - diffusers
    - git+https://github.com/0cc4m/hf_bleeding_edge/
    - https://github.com/0cc4m/GPTQ-for-LLaMa/archive/refs/tags/0.0.6.tar.gz
    - https://github.com/PanQiWei/AutoGPTQ/releases/download/v0.4.2/auto_gptq-0.4.2+cu118-cp310-cp310-linux_x86_64.whl; sys_platform == 'linux'
    - https://github.com/PanQiWei/AutoGPTQ/releases/download/v0.4.2/auto_gptq-0.4.2+cu118-cp310-cp310-win_amd64.whl; sys_platform == 'win32'
    - https://github.com/casper-hansen/AutoAWQ/releases/download/v0.1.4/autoawq-0.1.4-cp310-cp310-win_amd64.whl; sys_platform == 'win32' and python_version == '3.10'
    - https://github.com/casper-hansen/AutoAWQ/releases/download/v0.1.4/autoawq-0.1.4-cp310-cp310-linux_x86_64.whl; sys_platform == 'linux' and python_version == '3.10'
    - einops
    - peft==0.3.0
    - scipy
    - https://github.com/0cc4m/exllama/archive/refs/tags/0.0.7.tar.gz
    - exllamav2==0.0.7; sys_platform == 'linux'
    - git+https://github.com/turboderp/exllamav2.git; sys_platform == 'win32'
    - windows-curses; sys_platform == 'win32'
    - pynvml
    - xformers==0.0.21
    - https://github.com/Dao-AILab/flash-attention/releases/download/v2.3.0/flash_attn-2.3.0+cu118torch2.0cxx11abiFALSE-cp310-cp310-linux_x86_64.whl; sys_platform == 'linux'
    - omegaconf
